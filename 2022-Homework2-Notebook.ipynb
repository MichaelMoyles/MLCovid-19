{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework2\n",
    "\n",
    "Please note that this is a group project (2 students per group). If you do not have a group yet, please email georgiana.ifrim@ucd.ie to be assigned to a group.\n",
    "\n",
    "Please upload to Brightspace a **.zip** archive containing your Jupyter Notebook with solutions and all data required to reproduce your solutions. \n",
    "\n",
    "Please also prepare a **requirements.txt** file which lists all the packages that you have used for your homework, one package per line. This will allow us to install all required packages.\n",
    "\n",
    "Please name your .zip archive using your full names and student ids as follows - **Firstname1_Lastname1_id1_Firstname2_Lastname2_id2_COMP47350_Homework1.zip**. Please only 1 person in the team submit this zip file to Brightspace (no need for each member to submit a duplicate of the zip).\n",
    "\n",
    "For your Notebook, please split the code and explanations into cells so it is easy to see and read the results of each step of your solution. Please remember to name your variables and methods with self-explanatory names. Please remember to write comments and where needed, justifications, for the decisions you make and code you write. \n",
    "\n",
    "Your code and analysis is like a story that awaits to be read, please make it a nice and clear story. Always start with an introduction about the problem and your understanding of the problem domain and data analytics solution and describe your steps and your findings from each step.\n",
    "\n",
    "The accepted file formats for the homework are:\n",
    "    - .ipynb\n",
    "    - .zip\n",
    "    - .pdf\n",
    "    - .csv\n",
    "    \n",
    "Please aim to keep the whole code for Homework2 in a single notebook. In case you need to first prepare the data and prefer to do this in a separate notebook, before starting on Homework2, you can do this, but to the Homework2 notebook add a short summary of what was done to prepare the data. Please document the structure of your files in a README.txt file, so your submission folder and files are easy to navigate for an outsider.\n",
    "\n",
    "Usage of external tools/files is discouraged for portability reasons. Files in any other format but mentioned above can be used but will not considered for the submission (including .doc, .rar, .7z, .pages, .xlsx, .tex etc.). \n",
    "Any image format is allowed to be used as far as the images appear embedded in your report (.ipynb or .pdf or .html).\n",
    "\n",
    "**Deadline: Monday, 8 May, 2023, midnight.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**This homework focuses on training and evaluating prediction models for a particular problem and dataset.**\n",
    "The data comes from the Centers for Disease Control and Prevention (CDC: https://covid.cdc.gov/covid-data-tracker/). CDC is a USA health protection agency and is in charge of collecting data about the COVID-19 pandemic, and in particular, tracking cases, deaths, and trends of COVID-19 in the United States. CDC collects and makes public deidentified individual-case data on a daily basis, submitted using standardized case reporting forms. In this analysis, we focus on using the data collected by CDC to build a data analytics solution for death risk prediction. \n",
    "\n",
    "The dataset we work with is a sample of the public data released by CDC, where the outcome for the target feature **death_yn** is known (i.e., either 'yes' or 'no'):\n",
    "https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf\n",
    "\n",
    "The goal in this homework is to work with the data to build and evaluate prediction models that capture the relationship between the descriptive features and the target feature **death_yn**. For this homework you are asked to use the same dataset allocated to you in Homework1 (you can use your cleaned/prepared CSV from Homework1 or start from the raw dataset, clean it according to concepts covered in the lectures/labs, then use it for training prediction models). To use the 2 individual files allocated for Homework1, you can merge them first, then clean the resulting dataset, before starting on Homework2 requirements.\n",
    " \n",
    "There are 5 parts for this homework. Each part has an indicative maximum percentage given in brackets, e.g., part (1) has a maximum of 25% shown as [25]. The total that can be achieved is 100.\n",
    "\n",
    "\n",
    "(1). [25] **Data Understanding and Preparation:** Exploring relationships between feature pairs and selecting/transforming promising features based on a given training set.\n",
    "\n",
    "    - (1.1) Split the dataset into two datasets: 70% training and 30% test. Keep the test set aside. \n",
    "    - (1.2) On the training set:\n",
    "        - Plot the correlations between all the continuous features (if any). Discuss what you observe in these plots.\n",
    "        - For each continuous feature, plot its interaction with the target feature (a plot for each pair of   continuous feature and target feature). Discuss what you observe from these plots, e.g., which continuous features seem to be better at predicting the target feature? Choose a subset of continuous features you find promising (if any). Justify your choices.\n",
    "        - For each categorical feature, plot its pairwise interaction with the target feature. Discuss what  knowledge you gain from these plots, e.g., which categorical features seem to be better at predicting the target feature? Choose a subset of categorical features you find promising (if any). Justify your choices.\n",
    "      \n",
    "    \n",
    "(2). [15] **Predictive Modeling:** Linear Regression.  \n",
    "\n",
    "    - (2.1) On the training set, train a linear regression model to predict the target feature, using only the  descriptive features selected in exercise (1) above. \n",
    "    - (2.2) Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model by analysing each coefficient and how it relates each input feature to the target feature).    \n",
    "    - (2.3) Print the predicted target feature value for the first 10 training examples. Threshold the predicted target feature value given by the linear regression model at 0.5, to get the predicted class for each example. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (2.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained on the training (70%) dataset. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated random train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings.\n",
    "    \n",
    "(3). [15] **Predictive Modeling:** Logistic Regression.  \n",
    "\n",
    "    - (3.1) On the training set, train a logistic regression model to predict the target feature, using the descriptive features selected in exercise (1) above.   \n",
    "    - (3.2) Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model).    \n",
    "    - (3.3) Print the predicted target feature value for the first 10 training examples. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (3.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained when using the training (70%) dataset for evaluation. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings.\n",
    "    \n",
    "    \n",
    "(4). [20] **Predictive Modeling:** Random Forest.  \n",
    "\n",
    "    - (4.1) On the training set, train a random forest model to predict the target feature, using the descriptive features selected in exercise (1) above.   \n",
    "    - (4.2) Can you interpret the random forest model? Discuss any knowledge you can gain in regard of the working of this model.   \n",
    "    - (4.3) Print the predicted target feature value for the first 10 training examples. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (4.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained when using the training (70%) dataset for evaluation. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and to the Random Forest out-of-sample error and discuss your findings.\n",
    "    \n",
    "(5). [25] **Improving Predictive Models.**\n",
    "\n",
    "    - (5.1) Which model of the ones trained above performs better at predicting the target feature? Is it more   accurate than a simple model that always predicts the majority class (i.e., if 'no' is the majority class in your dataset, the simple model always predicts 'no' for the target feature)? Justify your answers.\n",
    "    - (5.2) Summarise your understanding of the problem and of your predictive modeling results so far. Can you think of any new ideas to improve the best model so far (e.g., by using furher data prep such as: feature selection, feature re-scaling, creating new features, combining predictive models, or using other domain knowledge)? Please show how your ideas actually work in practice (with code), by training and evaluating your proposed models. Summarise your findings so far. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- This notebook will apply the cleaned dataset from Task1 and use it to create 3 different models to predict whether the features can predict the death from Covid-19\n",
    "\n",
    "$This homework will be broken down into 4 main parts:$\n",
    "1. We will review the dataset from homework one and decide on which features to use to build our model\n",
    "2. We will create a Linear Regression model and analyse\n",
    "3. We will create a Logistical Regression model and analyse\n",
    "4. We will create a Random Forest model and analyse\n",
    "5. We will then try to optimized each model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our two original datasets have been combined and cleaned (Task1) and is now imported as a starting point for this work.\n",
    "1. The accompanying data quality report from Task1 can be found attached as PDF as a background to this cleaned dataset;\n",
    "2. A summary of this plan can be seen in the table below;\n",
    "3. In addition a number of extra features were added (to to better capture the problem domain) that were not in the original dataset.\n",
    "    - $DelqEver : Measures if an entry during their history has ever been delinquent$\n",
    "    - DelqLast12M : Measures if an entry during the last 12 months has ever been delinquent\n",
    "    - PercentSatisfactoryTrades: Measure the Percentage of satisfactory trades\n",
    "    - NumTradesWBalance: Number of trades with a balance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of data quality plan:\n",
    "\n",
    "| Feature Names                      | Data Quality Issue                     | Handling Strategy                |\n",
    "|------------------------------------|----------------------------------------|----------------------------------|\n",
    "| case_month                         | None                                   | Do Nothing                       |\n",
    "| res_state                          | None                                   | Do Nothing                       |\n",
    "| state_fips_code                    | None                                   | Do Nothing                       |\n",
    "| res_county                         | Missing Values (6%)                    | Replace with 'Missing'           |\n",
    "| county_fips_code                   | Missing Values (6%)                    | Replace with 'Missing'           |\n",
    "| age_group                          | Missing Values (0.8%)                  | Replace with 'Missing'           |\n",
    "| sex                                | Missing Values (2%)                    | Impute based on data proportions |\n",
    "| race                               | Missing Values (23%)                   | Replace with 'Missing'           |\n",
    "| ethnicity                          | Missing Values (31%)                   | Replace with 'Missing'           |\n",
    "| case_onset_interval                | Constant Column                        | Drop                             |\n",
    "| case_process_specimen_interval     | Negative Values & Missing Values (56%) | Replace with mode                |\n",
    "| process                            | Missing Values (91%)                   | Drop                             |\n",
    "| exposure_yn                        | Constant Column                        | Drop                             |\n",
    "| current_status                     | None                                   | Do Nothing                       |\n",
    "| sympotom_status                    | Missing Values (53%)                   | Replace with 'Missing'           |\n",
    "| hosp_yn                            | Missing Values (33%)                   | Replace with 'Missing'           |\n",
    "| icu_yn                             | Missing Values (91%)                   | Drop                             |\n",
    "| death_yn                           | None                                   | Do Nothing                       |\n",
    "| underlying_conditions_yn           | Missing Values (91%)                   | Drop                             |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.1 Review, prepare and split the dataset into two datasets: 70% training and 30% test\n",
    "Here we will import cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_month</th>\n",
       "      <th>res_state</th>\n",
       "      <th>state_fips_code</th>\n",
       "      <th>res_county</th>\n",
       "      <th>county_fips_code</th>\n",
       "      <th>age_group</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>case_positive_specimen_interval</th>\n",
       "      <th>current_status</th>\n",
       "      <th>symptom_status</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>death_prob_sex_age</th>\n",
       "      <th>death_prob_medical</th>\n",
       "      <th>state_party</th>\n",
       "      <th>death_prob_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12</td>\n",
       "      <td>CA</td>\n",
       "      <td>6</td>\n",
       "      <td>RIVERSIDE</td>\n",
       "      <td>6065.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.7429577464788732</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>0.427450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01</td>\n",
       "      <td>NJ</td>\n",
       "      <td>34</td>\n",
       "      <td>GLOUCESTER</td>\n",
       "      <td>34015.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>2</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Symptomatic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.7429577464788732</td>\n",
       "      <td>0.8035019455252919</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>0.173905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11</td>\n",
       "      <td>TN</td>\n",
       "      <td>47</td>\n",
       "      <td>PUTNAM</td>\n",
       "      <td>47141.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Symptomatic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.7429577464788732</td>\n",
       "      <td>0.8035019455252919</td>\n",
       "      <td>Republican</td>\n",
       "      <td>0.324263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04</td>\n",
       "      <td>NY</td>\n",
       "      <td>36</td>\n",
       "      <td>WESTCHESTER</td>\n",
       "      <td>36119.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Symptomatic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.7429577464788732</td>\n",
       "      <td>0.8035019455252919</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>0.782269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03</td>\n",
       "      <td>TN</td>\n",
       "      <td>47</td>\n",
       "      <td>SUMNER</td>\n",
       "      <td>47165.0</td>\n",
       "      <td>65+ years</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Symptomatic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.6741793238608526</td>\n",
       "      <td>0.8035019455252919</td>\n",
       "      <td>Republican</td>\n",
       "      <td>0.720820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  case_month res_state  state_fips_code   res_county county_fips_code  \\\n",
       "0    2020-12        CA                6    RIVERSIDE           6065.0   \n",
       "1    2022-01        NJ               34   GLOUCESTER          34015.0   \n",
       "2    2020-11        TN               47       PUTNAM          47141.0   \n",
       "3    2020-04        NY               36  WESTCHESTER          36119.0   \n",
       "4    2020-03        TN               47       SUMNER          47165.0   \n",
       "\n",
       "   age_group     sex     race            ethnicity  \\\n",
       "0  65+ years    Male  Missing              Missing   \n",
       "1  65+ years    Male    White  Non-Hispanic/Latino   \n",
       "2  65+ years    Male    White  Non-Hispanic/Latino   \n",
       "3  65+ years    Male    Black  Non-Hispanic/Latino   \n",
       "4  65+ years  Female    White  Non-Hispanic/Latino   \n",
       "\n",
       "   case_positive_specimen_interval             current_status symptom_status  \\\n",
       "0                                0  Laboratory-confirmed case        Missing   \n",
       "1                                2  Laboratory-confirmed case    Symptomatic   \n",
       "2                                0  Laboratory-confirmed case    Symptomatic   \n",
       "3                                0  Laboratory-confirmed case    Symptomatic   \n",
       "4                                0  Laboratory-confirmed case    Symptomatic   \n",
       "\n",
       "   hosp_yn death_yn  death_prob_sex_age  death_prob_medical state_party  \\\n",
       "0  Missing      Yes  0.7429577464788732             Missing    Democrat   \n",
       "1      Yes      Yes  0.7429577464788732  0.8035019455252919    Democrat   \n",
       "2      Yes      Yes  0.7429577464788732  0.8035019455252919  Republican   \n",
       "3      Yes      Yes  0.7429577464788732  0.8035019455252919    Democrat   \n",
       "4      Yes      Yes  0.6741793238608526  0.8035019455252919  Republican   \n",
       "\n",
       "   death_prob_month  \n",
       "0          0.427450  \n",
       "1          0.173905  \n",
       "2          0.324263  \n",
       "3          0.782269  \n",
       "4          0.720820  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the cleaned csv\n",
    "df = pd.read_csv(\"covid19_cdc_cleaned_data.csv\", keep_default_na=True, delimiter=',', skipinitialspace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_month</th>\n",
       "      <th>res_state</th>\n",
       "      <th>state_fips_code</th>\n",
       "      <th>res_county</th>\n",
       "      <th>county_fips_code</th>\n",
       "      <th>age_group</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>case_positive_specimen_interval</th>\n",
       "      <th>current_status</th>\n",
       "      <th>symptom_status</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>death_prob_sex_age</th>\n",
       "      <th>death_prob_medical</th>\n",
       "      <th>state_party</th>\n",
       "      <th>death_prob_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>2022-02</td>\n",
       "      <td>MN</td>\n",
       "      <td>27</td>\n",
       "      <td>POLK</td>\n",
       "      <td>27119.0</td>\n",
       "      <td>0 - 17 years</td>\n",
       "      <td>Female</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0003727171077152441</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>0.132992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>2022-11</td>\n",
       "      <td>OH</td>\n",
       "      <td>39</td>\n",
       "      <td>FRANKLIN</td>\n",
       "      <td>39049.0</td>\n",
       "      <td>18 to 49 years</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>0.005774216356351638</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Republican</td>\n",
       "      <td>0.007246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>2021-10</td>\n",
       "      <td>MI</td>\n",
       "      <td>26</td>\n",
       "      <td>WAYNE</td>\n",
       "      <td>26163.0</td>\n",
       "      <td>18 to 49 years</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Symptomatic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.005774216356351638</td>\n",
       "      <td>0.05534841329024774</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>0.144886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>2021-09</td>\n",
       "      <td>IN</td>\n",
       "      <td>18</td>\n",
       "      <td>ALLEN</td>\n",
       "      <td>18003.0</td>\n",
       "      <td>50 to 64 years</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>Non-Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Missing</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.09107413010590015</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Republican</td>\n",
       "      <td>0.189514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>2020-11</td>\n",
       "      <td>MD</td>\n",
       "      <td>24</td>\n",
       "      <td>PRINCE GEORGE'S</td>\n",
       "      <td>24033.0</td>\n",
       "      <td>50 to 64 years</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>0</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Symptomatic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.22256664600123993</td>\n",
       "      <td>0.05534841329024774</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>0.324263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      case_month res_state  state_fips_code       res_county county_fips_code  \\\n",
       "39995    2022-02        MN               27             POLK          27119.0   \n",
       "39996    2022-11        OH               39         FRANKLIN          39049.0   \n",
       "39997    2021-10        MI               26            WAYNE          26163.0   \n",
       "39998    2021-09        IN               18            ALLEN          18003.0   \n",
       "39999    2020-11        MD               24  PRINCE GEORGE'S          24033.0   \n",
       "\n",
       "            age_group     sex     race            ethnicity  \\\n",
       "39995    0 - 17 years  Female  Missing              Missing   \n",
       "39996  18 to 49 years  Female    Black  Non-Hispanic/Latino   \n",
       "39997  18 to 49 years  Female    Black  Non-Hispanic/Latino   \n",
       "39998  50 to 64 years  Female    White  Non-Hispanic/Latino   \n",
       "39999  50 to 64 years    Male    White      Hispanic/Latino   \n",
       "\n",
       "       case_positive_specimen_interval             current_status  \\\n",
       "39995                                0  Laboratory-confirmed case   \n",
       "39996                                0  Laboratory-confirmed case   \n",
       "39997                                0  Laboratory-confirmed case   \n",
       "39998                                0  Laboratory-confirmed case   \n",
       "39999                                0  Laboratory-confirmed case   \n",
       "\n",
       "      symptom_status  hosp_yn death_yn     death_prob_sex_age  \\\n",
       "39995        Missing  Missing       No  0.0003727171077152441   \n",
       "39996        Missing  Missing       No   0.005774216356351638   \n",
       "39997    Symptomatic       No       No   0.005774216356351638   \n",
       "39998        Missing       No       No    0.09107413010590015   \n",
       "39999    Symptomatic       No       No    0.22256664600123993   \n",
       "\n",
       "        death_prob_medical state_party  death_prob_month  \n",
       "39995              Missing    Democrat          0.132992  \n",
       "39996              Missing  Republican          0.007246  \n",
       "39997  0.05534841329024774    Democrat          0.144886  \n",
       "39998              Missing  Republican          0.189514  \n",
       "39999  0.05534841329024774    Democrat          0.324263  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned csv shape, datatypes will be inspected. We will also check for any remaining null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 18)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 18 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   case_month                       40000 non-null  object \n",
      " 1   res_state                        40000 non-null  object \n",
      " 2   state_fips_code                  40000 non-null  int64  \n",
      " 3   res_county                       40000 non-null  object \n",
      " 4   county_fips_code                 40000 non-null  object \n",
      " 5   age_group                        40000 non-null  object \n",
      " 6   sex                              40000 non-null  object \n",
      " 7   race                             40000 non-null  object \n",
      " 8   ethnicity                        40000 non-null  object \n",
      " 9   case_positive_specimen_interval  40000 non-null  int64  \n",
      " 10  current_status                   40000 non-null  object \n",
      " 11  symptom_status                   40000 non-null  object \n",
      " 12  hosp_yn                          40000 non-null  object \n",
      " 13  death_yn                         40000 non-null  object \n",
      " 14  death_prob_sex_age               40000 non-null  object \n",
      " 15  death_prob_medical               40000 non-null  object \n",
      " 16  state_party                      40000 non-null  object \n",
      " 17  death_prob_month                 40000 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(15)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_month                         0\n",
       "res_state                          0\n",
       "state_fips_code                    0\n",
       "res_county                         0\n",
       "county_fips_code                   0\n",
       "age_group                          0\n",
       "sex                                0\n",
       "race                               0\n",
       "ethnicity                          0\n",
       "case_positive_specimen_interval    0\n",
       "current_status                     0\n",
       "symptom_status                     0\n",
       "hosp_yn                            0\n",
       "death_yn                           0\n",
       "death_prob_sex_age                 0\n",
       "death_prob_medical                 0\n",
       "state_party                        0\n",
       "death_prob_month                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert features to appropriate datatypes\n",
    "$We will now review the datatypes and convert if needed. This will help avoid plotting errors later in the notebook$\n",
    "- The target feature \"Risk Performance\" is type object, with values \"Good\" & \"Bad\". These will be mapped 'Good': 1, \"Bad\": 0 and stored as \"int64\"\n",
    "- Categorical features \"DelqEver\" & \"DelqLast12M\" are boolean False/True values. These will be converted to \"int64\" with value of 0,1\n",
    "- Some Continuous features are float64 type:\n",
    "    - PercentSatisfactoryTrades is a derived feature from assignment 1 and is a float64. This can be converted to int64 without losing any information as the values were already rounded \n",
    "    - NumBank2NatlTradesWHighUtilization was an int64 and was cast to a float64 when performing imputation. It will now be converted back to int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name: case_month \n",
      "Data Type: object \n",
      "Number of Unique Values: 35\n",
      "['2020-12' '2022-01' '2020-11' '2020-04' '2020-03' '2021-08' '2021-07'\n",
      " '2022-02' '2021-11' '2022-06' '2020-07' '2021-01' '2020-05' '2021-10'\n",
      " '2020-06' '2020-08' '2021-12' '2020-09' '2020-10' '2021-03' '2022-10'\n",
      " '2021-04' '2021-09' '2021-02' '2022-05' '2020-01' '2022-07' '2021-05'\n",
      " '2022-11' '2022-03' '2022-08' '2022-09' '2022-04' '2021-06' '2020-02']\n",
      "\n",
      "Column Name: res_state \n",
      "Data Type: object \n",
      "Number of Unique Values: 49\n",
      "['CA' 'NJ' 'TN' 'NY' 'FL' 'MI' 'KY' 'MA' 'IL' 'WI' 'SC' 'TX' 'OH' 'NV'\n",
      " 'ND' 'AR' 'AZ' 'NH' 'PR' 'IN' 'MO' 'CT' 'AL' 'KS' 'UT' 'PA' 'OK' 'CO'\n",
      " 'OR' 'VA' 'MN' 'MD' 'LA' 'MT' 'NC' 'NM' 'GA' 'MS' 'ME' 'WA' 'ID' 'DC'\n",
      " 'IA' 'RI' 'WY' 'AK' 'VT' 'VI' 'GU']\n",
      "\n",
      "Column Name: state_fips_code \n",
      "Data Type: int64 \n",
      "Number of Unique Values: 49\n",
      "[ 6 34 47 36 12 26 21 25 17 55 45 48 39 32 38  5  4 33 72 18 29  9  1 20\n",
      " 49 42 40  8 41 51 27 24 22 30 37 35 13 28 23 53 16 11 19 44 56  2 50 78\n",
      " 66]\n",
      "\n",
      "Column Name: res_county \n",
      "Data Type: object \n",
      "Number of Unique Values: 930\n",
      "['RIVERSIDE' 'GLOUCESTER' 'PUTNAM' 'WESTCHESTER' 'SUMNER' 'LEE' 'BROWARD'\n",
      " 'LOS ANGELES' 'MACOMB' 'KERN' 'Missing' 'TULARE' 'BAY' 'MIDDLESEX' 'COOK'\n",
      " 'DUNN' 'SOMERSET' 'ORANGEBURG' 'BEXAR' 'BRISTOL' 'WAYNE' 'CLARK' 'CASS'\n",
      " 'SALINE' 'KINGS' 'MARICOPA' 'LUCAS' 'KNOX' 'FRANKLIN' 'SAN BERNARDINO'\n",
      " 'LAKE' 'PALM BEACH' 'WASHOE' 'STANISLAUS' 'DUPAGE' 'HILLSBOROUGH'\n",
      " 'MARION' 'ASHTABULA' 'PIMA' 'HAMPDEN' 'HENDRICKS' 'ORANGE' 'PINELLAS'\n",
      " 'FAIRFIELD' 'MONTGOMERY' 'GREENVILLE' 'CAMDEN' 'AUTAUGA' 'BUTTE'\n",
      " 'FLORENCE' 'ROCKINGHAM' 'ESSEX' 'UTAH' 'OCEAN' 'GRANT' 'OAKLAND' 'COMAL'\n",
      " 'TARRANT' 'HIGHLANDS' 'MIAMI-DADE' 'YORK' 'PULASKI' 'WESTMORELAND'\n",
      " 'BUTLER' 'WORCESTER' 'PINAL' 'OKLAHOMA' 'JACKSON' 'DELAWARE' 'QUEENS'\n",
      " 'ALLEN' 'EL PASO' 'HARRIS' 'DUVAL' 'MADISON' 'WOOD' 'JEFFERSON' 'PASSAIC'\n",
      " 'CUYAHOGA' 'PEORIA' 'SUMMIT' 'MULTNOMAH' 'SHELBY' 'INDIAN RIVER'\n",
      " 'PLYMOUTH' 'HENNEPIN' 'MCLEAN' 'SEBASTIAN' 'CONTRA COSTA' 'RUTHERFORD'\n",
      " 'OCONEE' 'FAYETTE' 'MONMOUTH' 'SARASOTA' 'SALT LAKE' 'BALTIMORE'\n",
      " 'TERREBONNE' 'YELLOWSTONE' 'HARTFORD' 'SPARTANBURG' 'NASSAU' 'CHESTER'\n",
      " 'NEW HANOVER' 'BERGEN' 'NORFOLK' 'CARTER' 'WRIGHT' 'YUMA' 'SAN JUAN'\n",
      " 'HAMILTON' 'SUFFOLK' 'BERKS' 'DALLAS' 'ERIE' 'MILWAUKEE' 'MOBILE'\n",
      " 'ALLEGHENY' 'DO�A ANA' 'LANCASTER' 'ST. JOSEPH' 'MEDINA' 'DANE'\n",
      " 'PHILADELPHIA' 'SHAWNEE' 'INGHAM' 'HANCOCK' 'HIDALGO' 'NEW HAVEN'\n",
      " 'HERNANDO' 'COCHISE' 'PITTSBURG' 'OKALOOSA' 'MANATEE' 'INDEPENDENCE'\n",
      " 'UNION' 'POLK' 'RICE' 'RICHMOND' 'SAN DIEGO' 'TRAVIS' 'MAHONING' 'CLAY'\n",
      " 'RENO' 'BOYLE' 'LEXINGTON' 'HOUSTON' 'LYCOMING' \"PRINCE GEORGE'S\" 'FLOYD'\n",
      " 'TUSCARAWAS' 'ORLEANS' 'MAVERICK' 'SANTA CLARA' 'CAMBRIA' 'IMPERIAL'\n",
      " 'ANOKA' 'GREENE' 'AROOSTOOK' 'KENOSHA' 'MECKLENBURG' 'LEON' 'GRADY'\n",
      " 'BURLINGTON' 'YAVAPAI' 'TULSA' 'ST. LOUIS' 'MARTIN' 'CRAWFORD' 'MCLENNAN'\n",
      " 'CATAWBA' 'HUDSON' 'LAWRENCE' 'FREDERICK' 'COLUMBIANA' 'KING' 'ADAMS'\n",
      " 'DORCHESTER' 'DENVER' 'GENESEE' 'SEDGWICK' 'SAN JOAQUIN' 'NEW YORK'\n",
      " 'SPOKANE' 'CITRUS' 'MERCED' 'DAVIDSON' 'SULLIVAN' 'VOLUSIA' 'MOHAVE'\n",
      " 'KENT' 'KLAMATH' 'OSCEOLA' 'LANE' 'BRONX' 'SNOHOMISH' 'WAUKESHA' 'PASCO'\n",
      " 'FRESNO' 'STARK' 'WILL' 'SACRAMENTO' 'SANTA BARBARA' 'LUZERNE'\n",
      " 'BARNSTABLE' 'CADDO' 'JOHNSON' 'THURSTON' 'WAKE' 'CALCASIEU' 'ALAMEDA'\n",
      " 'BERNALILLO' 'COLUMBIA' 'SANTA CRUZ' 'SAN LUIS OBISPO' 'BREVARD' 'MERCER'\n",
      " 'ARAPAHOE' 'SANGAMON' 'MCHENRY' 'BROWN' 'PORTER' 'ROGERS' 'MACOUPIN'\n",
      " 'RICHLAND' 'DAKOTA' 'WINNEBAGO' 'COLLIN' 'LOGAN' 'LICKING' 'SAGINAW'\n",
      " 'CANYON' 'SEMINOLE' 'ROCKLAND' 'LUBBOCK' 'MONROE' 'HARFORD' 'CAPE MAY'\n",
      " 'KOOTENAI' 'SHIAWASSEE' 'WASHINGTON' 'DISTRICT OF COLUMBIA' 'SONOMA'\n",
      " 'BUCKS' 'SCIOTO' 'HORRY' 'NAVAJO' 'BOONE' 'OUTAGAMIE' 'CUMBERLAND'\n",
      " 'WARREN' 'CHARLOTTE' 'BENTON' 'ELKHART' 'LEHIGH' 'ST. JOHNS'\n",
      " 'CAPE GIRARDEAU' 'KANE' 'CLERMONT' 'ST. CLAIR' \"ST. MARY'S\" 'SHASTA'\n",
      " 'MORRIS' 'WILKES' 'WASHTENAW' 'RAMSEY' 'ST. LUCIE' 'NUECES' 'RACINE'\n",
      " 'MUSKOGEE' 'LEWIS AND CLARK' 'HARNETT' 'CHESTERFIELD' 'LYNCHBURG CITY'\n",
      " 'ONTARIO' 'BLOUNT' 'PICKENS' 'LARIMER' 'ANDERSON' 'LAGRANGE' 'BERKSHIRE'\n",
      " 'INDIANA' 'MESA' 'VENTURA' 'MUSKINGUM' 'GRAND FORKS' 'MARINETTE'\n",
      " 'POTTAWATOMIE' 'MANITOWOC' 'VANDERBURGH' 'ROCK ISLAND' 'CLAIBORNE'\n",
      " 'LEBANON' 'CAMPBELL' 'CHARLESTON' 'BOULDER' 'DARLINGTON' 'ATLANTIC'\n",
      " 'NORTHAMPTON' 'GASTON' 'VIRGINIA BEACH CITY' 'PIERCE' 'OGLE' 'LAPEER'\n",
      " 'CARSON CITY' 'EAST BATON ROUGE' 'LOUDOUN' 'STEPHENS' 'ESCAMBIA'\n",
      " 'ST. TAMMANY' 'SUSSEX' 'SUMTER' 'TAZEWELL' 'GEAUGA' 'ETOWAH' 'IOSCO'\n",
      " 'SEVIER' 'WELD' 'MONTEREY' 'ROCKBRIDGE' 'CHIPPEWA' 'BARRY' 'YAKIMA'\n",
      " 'PLACER' 'TIPPECANOE' 'RICHMOND CITY' 'TUSCALOOSA' 'CALDWELL' 'VAN WERT'\n",
      " 'MACON' 'OTTER TAIL' 'LORAIN' 'SMYTH' 'MARIN' 'TRUMBULL' 'PRINCE WILLIAM'\n",
      " 'BONNEVILLE' 'MUSCATINE' 'OTTAWA' 'PICKAWAY' 'TANGIPAHOA' 'PROVIDENCE'\n",
      " 'NORTHUMBERLAND' 'BALDWIN' 'IREDELL' 'RAPIDES' 'LACKAWANNA' 'HENRY'\n",
      " 'HUNTERDON' 'ANDROSCOGGIN' 'MIFFLIN' 'GUILFORD' 'SILVER BOW' 'JASPER'\n",
      " 'MCKINLEY' 'DAUPHIN' 'CRAIGHEAD' 'LAFAYETTE' 'NEW LONDON' 'BUNCOMBE'\n",
      " 'MERRIMACK' 'BERRIEN' 'HAMPSHIRE' 'COLLIER' 'HOWARD' 'SCHUYLKILL'\n",
      " 'FAIRFAX' 'CLEVELAND' 'FOND DU LAC' 'ARLINGTON' 'SENECA' 'PERSON'\n",
      " 'DOUGLAS' 'LONOKE' 'SURRY' 'CLACKAMAS' 'ONONDAGA' 'ADA'\n",
      " 'MATANUSKA-SUSITNA' 'WEBSTER' 'CARROLL' 'STEARNS' 'HARDIN' 'GARLAND'\n",
      " 'LARAMIE' 'BRAZORIA' 'SANTA ROSA' 'DODGE' 'ST. CHARLES' 'WEBB'\n",
      " 'BALTIMORE CITY' 'LOUDON' 'TALLAPOOSA' 'FLAGLER' 'SAN FRANCISCO'\n",
      " 'NEWPORT NEWS CITY' 'DECATUR' 'MIAMI' 'OVERTON' 'WILLIAMS' 'CHRISTIAN'\n",
      " 'PARKER' 'WARD' 'DEKALB' 'HARVEY' 'WALKER' 'AIKEN' 'GILA' 'FULTON'\n",
      " 'BEAVER' 'FORSYTH' 'KENTON' 'KANKAKEE' 'BRADFORD' 'ROWAN' 'PLATTE'\n",
      " 'GIBSON' 'GRATIOT' 'WILSON' 'GARFIELD' 'TUSCOLA' 'CASCADE' 'WAUPACA'\n",
      " 'OLMSTED' 'PUEBLO' 'MAURY' 'BARTON' 'MONTROSE' 'PORTAGE' 'TEHAMA'\n",
      " 'BANNOCK' 'ALLEGANY' 'ANNE ARUNDEL' 'LASALLE' 'BRUNSWICK' 'HAYWOOD'\n",
      " 'HURON' 'CLINTON' 'NYE' 'KAUFMAN' 'WILLIAMSON' 'ST. LOUIS CITY' 'BLAIR'\n",
      " 'WARRICK' 'HENDERSON' 'DAVIS' 'NATRONA' 'ARMSTRONG' 'MORGAN' 'TIOGA'\n",
      " 'HENRICO' 'OUACHITA' 'BONNER' 'CHAVES' 'POTTAWATTAMIE' 'MUSKEGON'\n",
      " 'MORTON' 'TWIN FALLS' 'MOORE' 'SAN MATEO' 'SALEM' 'LAURENS'\n",
      " 'ROANOKE CITY' 'ALBANY' 'SAMPSON' 'OTSEGO' 'NIAGARA' 'RANDOLPH' 'WEBER'\n",
      " 'DURHAM' 'DUTCHESS' 'HUNTINGTON' 'WYOMING' 'MORRISON' 'LINCOLN'\n",
      " 'GRANVILLE' 'SCOTLAND' 'CAYUGA' 'ROCK' 'LEAVENWORTH' 'DESCHUTES'\n",
      " 'HALIFAX' 'JENNINGS' 'BEAUFORT' 'ROBESON' 'PREBLE' 'OSWEGO' 'WICOMICO'\n",
      " 'ST. CROIX' 'STANLY' 'BEDFORD' 'PORTSMOUTH CITY' 'CARLTON' 'MUHLENBERG'\n",
      " 'OLDHAM' 'RIPLEY' 'BECKER' 'ONEIDA' 'MAYES' 'CALLOWAY' 'POTTER' 'LAUREL'\n",
      " 'TODD' 'BLUE EARTH' 'CHESAPEAKE CITY' 'NICOLLET' 'SCHENECTADY'\n",
      " 'TRANSYLVANIA' 'LETCHER' 'STEELE' 'STOKES' 'CARVER' 'CHAUTAUQUA'\n",
      " 'MARSHALL' 'COCONINO' 'WHITE' 'COMANCHE' 'WYANDOTTE' 'CABARRUS' 'CHARLES'\n",
      " 'LIVINGSTON' 'BROOME' 'KANDIYOHI' 'PENDER' 'WICHITA' 'CENTRE' 'HOLMES'\n",
      " 'ST. LAWRENCE' 'CHEROKEE' 'LYON' 'SCOTT' 'WABASHA' 'CORTLAND' 'CURRY'\n",
      " 'EAU CLAIRE' 'PENOBSCOT' 'HAMPTON CITY' 'OXFORD' 'COOS' 'RILEY'\n",
      " 'MILLE LACS' 'COWLEY' 'DALE' 'PAYNE' 'CHARLOTTESVILLE CITY' 'GREENUP'\n",
      " 'CUSTER' 'RENSSELAER' 'ONSLOW' 'BELL' 'DAVIESS' 'GALLIA'\n",
      " 'HARRISONBURG CITY' 'JAY' 'WELLS' 'WINONA' 'CAMERON' 'BULLITT' 'MIDLAND'\n",
      " 'KOSCIUSKO' 'GREENWOOD' 'HOOD RIVER' 'GONZALES' 'BARREN' 'SHERBURNE'\n",
      " 'PITT' 'CECIL' 'STEUBEN' 'VIGO' 'ALAMANCE' 'FAUQUIER' 'NELSON' 'LINN'\n",
      " 'EMMET' 'BENNINGTON' 'BELKNAP' 'HARDEE' 'VAN BUREN' 'DEFIANCE' 'WOODBURY'\n",
      " 'COLUMBUS' 'YADKIN' 'FLATHEAD' 'HOKE' 'GRAY' 'OSAGE' 'UMATILLA'\n",
      " 'CURRITUCK' 'ADDISON' 'RUTLAND' 'TOMPKINS' 'HAMBLEN' 'HOCKING' 'GARVIN'\n",
      " 'HOPKINS' 'NASH' 'MISSOULA' 'JONES' 'JOSEPHINE' 'BOYD' 'TAYLOR'\n",
      " 'CULPEPER' 'BURKE' 'CHISAGO' 'GALLATIN' 'NEWPORT' 'ASHLAND' 'HANOVER'\n",
      " 'PONTOTOC' 'ULSTER' 'JOHNSTON' 'WOODFORD' 'NORFOLK CITY' 'JEROME' 'DYER'\n",
      " 'SARATOGA' 'CHAMBERS' 'LENAWEE' 'POPE' 'CHATHAM' 'MANASSAS CITY'\n",
      " 'FORT BEND' 'BARTHOLOMEW' 'DENTON' 'STAFFORD' 'WILLACY' 'BRYAN'\n",
      " 'MARATHON' 'BERKELEY' 'ITASCA' 'RANDALL' 'WHITLEY' 'WATAUGA' 'ASHE'\n",
      " 'EDGECOMBE' 'FILLMORE' 'ANSON' 'YAMHILL' 'GRAYSON' 'ISABELLA' 'MCPHERSON'\n",
      " 'LEWIS' 'CHEMUNG' 'ALBEMARLE' 'SAGADAHOC' 'CREEK' 'WINDHAM' 'MOWER'\n",
      " 'ANTRIM' 'CORYELL' 'SPOTSYLVANIA' 'DUPLIN' 'WEXFORD' 'WAKULLA'\n",
      " 'COVINGTON' 'DELTA' 'MARQUETTE' 'SEWARD' 'CALUMET' 'CROW WING'\n",
      " 'HILLSDALE' 'DEARBORN' 'BLAINE' 'CATTARAUGUS' 'SUFFOLK CITY' 'GRIMES'\n",
      " 'FORD' 'EATON' 'POWHATAN' 'ALLEGAN' 'SWEETWATER' 'LENOIR' 'MCMINN' 'ELKO'\n",
      " 'CASSIA' 'ECTOR' 'CLARENDON' 'DAVIE' 'SCHOHARIE' 'GEORGETOWN' 'HOCKLEY'\n",
      " 'BELTRAMI' 'CLEBURNE' 'ALACHUA' 'DUBOIS' 'HERKIMER' 'TALBOT' 'VILAS'\n",
      " 'BRADLEY' 'CRAVEN' 'ALEXANDRIA CITY' 'COFFEE' 'MANISTEE' 'LE SUEUR'\n",
      " 'WYTHE' 'JESSAMINE' 'ROANOKE' 'NOBLE' 'ROSS' 'BLADEN' 'ROBERTSON' 'EAGLE'\n",
      " 'ELMORE' 'ST. FRANCIS' 'CLATSOP' 'MASON' 'GADSDEN' 'KENNEBEC' 'WISE'\n",
      " 'GREEN' 'PINE' 'COLLETON' 'DANVILLE CITY' 'TIPTON' 'SAUK' 'MONTCALM'\n",
      " 'LAPORTE' 'NEWAYGO' 'IONIA' 'BINGHAM' 'CALHOUN' 'CAROLINE' 'COCKE'\n",
      " 'BRAZOS' 'ALEXANDER' 'ATHENS' 'SHENANDOAH' 'CARTERET' 'SANDUSKY' 'MCLEOD'\n",
      " 'STARR' 'BOX ELDER' 'NEZ PERCE' 'FINNEY' 'WAGONER' 'MEEKER' 'WALTON'\n",
      " 'NAVARRO' 'MCKEAN' 'LEVY' 'ADAIR' 'WINDSOR' 'WABASH' 'ISANTI' 'GUERNSEY'\n",
      " 'CHITTENDEN' 'BARRON' 'MCDOWELL' 'ROCKWALL' 'PERRY' 'NOBLES' 'CHENANGO'\n",
      " 'RUSSELL' 'ELLIS' 'DARKE' 'FORREST' 'FREEBORN' 'CHEATHAM' 'ISLE OF WIGHT'\n",
      " 'STARKE' 'COSHOCTON' 'STRAFFORD' 'HOUGHTON' 'HINDS' 'LOUISA' 'CANADIAN'\n",
      " 'WYANDOT' 'VICTORIA' 'HARLAN' 'GOOCHLAND' 'PITTSYLVANIA' 'BAKER'\n",
      " 'CALVERT' 'LA CROSSE' 'GRAVES' 'IRON' \"QUEEN ANNE'S\" 'OCONTO' 'SIOUX'\n",
      " 'WASATCH' 'RAVALLI' 'LITCHFIELD' 'CHESHIRE' 'GRAFTON' 'MCCRACKEN'\n",
      " 'CRITTENDEN' 'BELMONT' 'VANCE' 'DILLON' 'TILLAMOOK' 'GEARY' 'OWEN'\n",
      " 'BOTETOURT' 'CACHE' 'PIKE' 'HAYS' 'OZAUKEE' 'HIGHLAND' 'BAXTER'\n",
      " 'CHURCHILL' 'ROANE' 'GOODHUE' 'KERSHAW' 'GRAHAM' 'CHAMPAIGN' 'WALDO'\n",
      " 'AUGLAIZE' 'BRANCH' 'MENOMINEE' 'RANKIN' 'FREMONT' 'VENANGO' 'PASQUOTANK'\n",
      " 'BEE' 'SEQUOYAH' 'MISSISSIPPI' 'KALAMAZOO' 'WASCO' 'MILLER' 'OHIO'\n",
      " 'MINIDOKA' 'WALWORTH' 'BECKHAM' 'LAMOILLE' 'WAYNESBORO CITY' 'MALHEUR'\n",
      " 'JAMES CITY' 'WAPELLO' 'HUTCHINSON' 'DICKINSON' 'DUBUQUE' 'CROOK' 'DARE'\n",
      " 'AMHERST' 'CASWELL' 'DINWIDDIE' 'DESOTO' 'MUSCOGEE' 'SALEM CITY' 'CARBON'\n",
      " 'UVALDE' 'AUGUSTA' 'WEAKLEY' 'PRINCE GEORGE' 'COOKE' 'MECOSTA' 'LAVACA'\n",
      " 'ANCHORAGE' 'CHARLEVOIX' 'SHEBOYGAN' 'MEADE' 'BURNET' 'HARRISON' 'CONWAY'\n",
      " 'MCCURTAIN' 'MORROW' 'SMITH' 'HUBBARD' 'CLEARFIELD' 'MARLBORO' 'MCCLAIN'\n",
      " 'OKMULGEE' 'FLUVANNA' 'HERTFORD' 'POSEY' 'ACCOMACK' 'LE FLORE' 'HENDRY'\n",
      " 'GRAND TRAVERSE' 'HOPEWELL CITY' 'JUNEAU' 'VALENCIA' 'CALAVERAS' 'COLES'\n",
      " 'EL DORADO' 'KITSAP' 'TREMPEALEAU' 'STEPHENSON' 'JERSEY' 'KAY'\n",
      " 'LAFOURCHE' 'SANTA FE' 'EFFINGHAM' 'TROUP' 'CULLMAN' 'LAUDERDALE'\n",
      " 'NEWBERRY' 'BUCHANAN' 'TOM GREEN' 'CIBOLA' 'OTERO' 'TETON' 'HARDEMAN'\n",
      " 'GALVESTON' 'FAULKNER' 'WINCHESTER CITY' 'STORY' 'THOMAS' 'VERNON' 'KERR'\n",
      " 'SPENCER' 'WAUSHARA' 'OCEANA' 'HOT SPRING' 'OGEMAW' 'LEELANAU' 'TYLER'\n",
      " 'EDGEFIELD' 'GUADALUPE' 'HOOD' 'BLACK HAWK' 'PAGE' 'YELL' 'HAWKINS'\n",
      " 'RUSK' 'CERRO GORDO' 'GLADWIN' 'TOOELE' 'SUSQUEHANNA' 'LAMAR' 'LATAH'\n",
      " 'MEIGS' 'WINSTON' 'NACOGDOCHES' 'SANPETE' 'KING GEORGE' 'ELK'\n",
      " 'PETERSBURG CITY' 'SUWANNEE' 'PARK' 'UINTAH']\n",
      "\n",
      "Column Name: county_fips_code \n",
      "Data Type: object \n",
      "Number of Unique Values: 1327\n",
      "['6065.0' '34015.0' '47141.0' ... '49047.0' '47079.0' '48253.0']\n",
      "\n",
      "Column Name: age_group \n",
      "Data Type: object \n",
      "Number of Unique Values: 5\n",
      "['65+ years' '50 to 64 years' '18 to 49 years' 'Missing' '0 - 17 years']\n",
      "\n",
      "Column Name: sex \n",
      "Data Type: object \n",
      "Number of Unique Values: 3\n",
      "['Male' 'Female' 'Other']\n",
      "\n",
      "Column Name: race \n",
      "Data Type: object \n",
      "Number of Unique Values: 7\n",
      "['Missing' 'White' 'Black' 'Asian' 'Multiple/Other'\n",
      " 'American Indian/Alaska Native' 'Native Hawaiian/Other Pacific Islander']\n",
      "\n",
      "Column Name: ethnicity \n",
      "Data Type: object \n",
      "Number of Unique Values: 3\n",
      "['Missing' 'Non-Hispanic/Latino' 'Hispanic/Latino']\n",
      "\n",
      "Column Name: case_positive_specimen_interval \n",
      "Data Type: int64 \n",
      "Number of Unique Values: 41\n",
      "[ 0  2  1  5 13  3  4 14  9 53  6 22 16  8 33 30 36 20  7 60 18 44 38 27\n",
      " 73 45 21 29 10 31 12 26 28 25 52 19 32 50 11 67 24]\n",
      "\n",
      "Column Name: current_status \n",
      "Data Type: object \n",
      "Number of Unique Values: 2\n",
      "['Laboratory-confirmed case' 'Probable Case']\n",
      "\n",
      "Column Name: symptom_status \n",
      "Data Type: object \n",
      "Number of Unique Values: 3\n",
      "['Missing' 'Symptomatic' 'Asymptomatic']\n",
      "\n",
      "Column Name: hosp_yn \n",
      "Data Type: object \n",
      "Number of Unique Values: 3\n",
      "['Missing' 'Yes' 'No']\n",
      "\n",
      "Column Name: death_yn \n",
      "Data Type: object \n",
      "Number of Unique Values: 2\n",
      "['Yes' 'No']\n",
      "\n",
      "Column Name: death_prob_sex_age \n",
      "Data Type: object \n",
      "Number of Unique Values: 9\n",
      "['0.7429577464788732' '0.6741793238608526' '0.22256664600123993'\n",
      " '0.019941775836972344' '0.09107413010590015' '0.005774216356351638'\n",
      " 'Missing' '0.0003727171077152441' '0.0']\n",
      "\n",
      "Column Name: death_prob_medical \n",
      "Data Type: object \n",
      "Number of Unique Values: 5\n",
      "['Missing' '0.8035019455252919' '0.05534841329024774'\n",
      " '0.11136890951276102' '0.723404255319149']\n",
      "\n",
      "Column Name: state_party \n",
      "Data Type: object \n",
      "Number of Unique Values: 3\n",
      "['Democrat' 'Republican' 'PR']\n",
      "\n",
      "Column Name: death_prob_month \n",
      "Data Type: float64 \n",
      "Number of Unique Values: 35\n",
      "[0.42744984 0.17390473 0.32426304 0.78226858 0.72082019 0.27147577\n",
      " 0.27309783 0.13299233 0.20684039 0.06551298 0.33365201 0.35148515\n",
      " 0.54242424 0.14488636 0.25411061 0.29903978 0.16935773 0.19055375\n",
      " 0.25042735 0.07894737 0.06849315 0.07551487 0.18951358 0.1839196\n",
      " 0.02629482 0.90909091 0.07948443 0.05292479 0.00724638 0.04424779\n",
      " 0.0396927  0.06153846 0.0230608  0.05555556 0.66666667]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display all unique values in each column\n",
    "columns = df.columns\n",
    "\n",
    "for column in columns:\n",
    "    print(\"Column Name:\", column, \"\\nData Type:\", df[column].dtype, \"\\nNumber of Unique Values:\", str(len(df[column].unique())))\n",
    "    print(df[column].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the string res_state and res_county as state_fips_code and count_fips_code \n",
    "# are already numeric values that correspond to the state and county respectively\n",
    "df.drop(columns=['res_state', 'res_county'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all object types to categorical datatypes\n",
    "object_columns = df.select_dtypes(['object']).columns\n",
    "for column in object_columns:\n",
    "    df[column] = df[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each categorical feature into an numeric value\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
